{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import gc\n",
    "import json\n",
    "import tarfile\n",
    "import codecs\n",
    "import sys \n",
    "import time\n",
    "import argparse\n",
    "import os\n",
    "\n",
    "from collections import defaultdict\n",
    "from mmnrm.text import TREC_goldstandard_transform, TREC_queries_transform\n",
    "from mmnrm.evaluation import TREC_Evaluator\n",
    "from mmnrm.dataset import TestCollectionV2, sentence_splitter_builderV2\n",
    "from mmnrm.evaluation import BioASQ_Evaluator\n",
    "from mmnrm.modelsv2 import deep_rank\n",
    "from mmnrm.utils import set_random_seed, load_model_weights, load_model\n",
    "\n",
    "\n",
    "\n",
    "def load_TREC_queries(file):\n",
    "    df = pd.read_csv(file, sep=\"\\t\")\n",
    "    df.columns = [\"id\", \"query\"]\n",
    "    topics = []\n",
    "    for _,l in df.iterrows():\n",
    "        topics.append({\"query\":str(l[\"query\"]), \"id\":str(l[\"id\"])})\n",
    "        \n",
    "    return TREC_queries_transform(topics, number_parameter=\"id\", fn=lambda x:x[\"query\"])\n",
    "\n",
    "def load_TREC_qrels(q_rels_file):\n",
    "    \n",
    "    with open(q_rels_file) as f:\n",
    "        goldstandard = defaultdict(list)\n",
    "\n",
    "        for line in f:\n",
    "            line = line.strip().split(\" \")\n",
    "            try:\n",
    "                goldstandard[line[0]].append((line[2], line[3]))\n",
    "            except :\n",
    "                print(line)\n",
    "            \n",
    "    return TREC_goldstandard_transform(goldstandard)\n",
    "\n",
    "def load_prerank(file_name, collection, top_k=100):\n",
    "    prerank = defaultdict(list)\n",
    "    min_rank = 999\n",
    "    with open(file_name) as f:\n",
    "        for line in f:\n",
    "            elements = line.split(\" \")\n",
    "            if elements[2] in collection and len(prerank[elements[0]])<top_k:\n",
    "                article = collection[elements[2]]\n",
    "                prerank[elements[0]].append({\"id\":elements[2], \n",
    "                                              \"score\":elements[4],\n",
    "                                              \"text\":article[\"text\"],\n",
    "                                              \"title\":article[\"title\"]})\n",
    "            \n",
    "       \n",
    "    \n",
    "    # create test collection base on the docs\n",
    "    #docs_per_topic = [len(docs_topic) for docs_topic in prerank.values()]\n",
    "    #print(\"average docs per topic\", sum(docs_per_topic)/len(docs_per_topic), \"min:\",min(docs_per_topic),\"max:\",max(docs_per_topic))\n",
    "    \n",
    "    return prerank\n",
    "\n",
    "def collection_iterator(file_name, f_map=None):\n",
    "    return collection_iterator_fn(file_name=file_name, f_map=f_map)()\n",
    "\n",
    "def collection_iterator_fn(file_name, f_map=None):\n",
    "    \n",
    "    reader = codecs.getreader(\"ascii\")\n",
    "    tar = tarfile.open(file_name)\n",
    "\n",
    "    print(\"[CORPORA] Openning tar file\", file_name)\n",
    "\n",
    "    members = tar.getmembers()\n",
    "    \n",
    "    def generator():\n",
    "        for m in members:\n",
    "            print(\"[CORPORA] Openning tar file {}\".format(m.name))\n",
    "            f = tar.extractfile(m)\n",
    "            articles = json.load(reader(f))\n",
    "            if f_map is not None:\n",
    "                articles = list(map(f_map, articles))\n",
    "            yield articles\n",
    "            f.close()\n",
    "            del f\n",
    "            gc.collect()\n",
    "    return generator\n",
    "\n",
    "def load_neural_model(path_to_weights):\n",
    "    \n",
    "    rank_model = load_model(path_to_weights, change_config={\"return_snippets_score\":True})\n",
    "    tk = rank_model.tokenizer\n",
    "    \n",
    "    model_cfg = rank_model.savable_config[\"model\"]\n",
    "    \n",
    "    max_input_query = model_cfg[\"max_q_length\"]\n",
    "    max_input_sentence = model_cfg[\"max_s_length\"]\n",
    "    max_s_per_q_term = model_cfg[\"max_s_per_q_term\"]\n",
    "    \n",
    "    # redundant code... replace\n",
    "    max_sentences_per_query = model_cfg[\"max_s_per_q_term\"]\n",
    "\n",
    "    pad_query = lambda x, dtype='int32': tf.keras.preprocessing.sequence.pad_sequences(x, \n",
    "                                                                                       maxlen=max_input_query,\n",
    "                                                                                       dtype=dtype, \n",
    "                                                                                       padding='post', \n",
    "                                                                                       truncating='post', \n",
    "                                                                                       value=0)\n",
    "\n",
    "    pad_sentences = lambda x, dtype='int32': tf.keras.preprocessing.sequence.pad_sequences(x, \n",
    "                                                                                           maxlen=max_input_sentence,\n",
    "                                                                                           dtype=dtype, \n",
    "                                                                                           padding='post', \n",
    "                                                                                           truncating='post', \n",
    "                                                                                           value=0)\n",
    "\n",
    "    pad_docs = lambda x, max_lim, dtype='int32': x[:max_lim] + [[]]*(max_lim-len(x))\n",
    "\n",
    "    idf_from_id_token = lambda x: math.log(tk.document_count/tk.word_docs[tk.index_word[x]])\n",
    "\n",
    "    train_sentence_generator, test_sentence_generator = sentence_splitter_builderV2(tk, \n",
    "                                                                                      max_sentence_size=max_input_sentence,\n",
    "                                                                                      mode=4)\n",
    "\n",
    "\n",
    "    def test_input_generator(data_generator):\n",
    "\n",
    "        data_generator = test_sentence_generator(data_generator)\n",
    "\n",
    "        for _id, query, docs in data_generator:\n",
    "\n",
    "            # tokenization\n",
    "            query_idf = list(map(lambda x: idf_from_id_token(x), query))\n",
    "\n",
    "            tokenized_docs = []\n",
    "            ids_docs = []\n",
    "            offsets_docs = []\n",
    "            for doc in docs:\n",
    "\n",
    "                padded_doc = pad_docs(doc[\"text\"], max_lim=max_input_query)\n",
    "                for q in range(len(padded_doc)):\n",
    "                    padded_doc[q] = pad_docs(padded_doc[q], max_lim=max_sentences_per_query)\n",
    "                    padded_doc[q] = pad_sentences(padded_doc[q])\n",
    "                tokenized_docs.append(padded_doc)\n",
    "                ids_docs.append(doc[\"id\"])\n",
    "                offsets_docs.append(doc[\"offset\"])\n",
    "\n",
    "            # padding\n",
    "            query = pad_query([query])[0]\n",
    "            query = [query] * len(tokenized_docs)\n",
    "            query_idf = pad_query([query_idf], dtype=\"float32\")[0]\n",
    "            query_idf = [query_idf] * len(tokenized_docs)\n",
    "\n",
    "            yield _id, [np.array(query), np.array(tokenized_docs), np.array(query_idf)], ids_docs, offsets_docs\n",
    "    \n",
    "    return rank_model, test_input_generator\n",
    "\n",
    "def rank(model, t_collection):\n",
    "\n",
    "    generator_Y = t_collection.generator()\n",
    "                \n",
    "    q_scores = defaultdict(list)\n",
    "\n",
    "    for i, _out in enumerate(generator_Y):\n",
    "        query_id, Y, docs_info, offsets_docs = _out\n",
    "        s_time = time.time()\n",
    "        \n",
    "        scores, q_sentence_attention = model.predict(Y)\n",
    "        scores = scores[:,0].tolist()\n",
    "            \n",
    "        print(\"\\rEvaluation {} | time {}\".format(i, time.time()-s_time), end=\"\\r\")\n",
    "        #q_scores[query_id].extend(list(zip(docs_ids,scores)))\n",
    "        for i in range(len(docs_info)):\n",
    "            q_scores[query_id].append((docs_info[i], scores[i], q_sentence_attention[i], offsets_docs[i]))\n",
    "\n",
    "    # sort the rankings\n",
    "    for query_id in q_scores.keys():\n",
    "        q_scores[query_id].sort(key=lambda x:-x[1])\n",
    "        q_scores[query_id] = q_scores[query_id]\n",
    "    \n",
    "    return q_scores\n",
    "\n",
    "def save_answers_to_file(answers, name):\n",
    "    _name = name.split(\".\")[0]+\"_answer.txt\"\n",
    "    \n",
    "    with open(_name,\"w\", encoding=\"utf-8\") as f:\n",
    "        for line in answers:\n",
    "            f.write(line+\"\\n\")\n",
    "        \n",
    "    return _name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CORPORA] Openning tar file /backup/MS-MARCO/ms-marco-docs.tar.gz\n",
      "[CORPORA] Openning tar file tmp/tmpvmkbfob_/ms-marco-docs_1000000_to_1500000\n",
      "[CORPORA] Openning tar file tmp/tmpvmkbfob_/ms-marco-docs_1500000_to_2000000\n",
      "[CORPORA] Openning tar file tmp/tmpvmkbfob_/ms-marco-docs_2000000_to_2500000\n",
      "[CORPORA] Openning tar file tmp/tmpvmkbfob_/ms-marco-docs_2500000_to_3000000\n",
      "[CORPORA] Openning tar file tmp/tmpvmkbfob_/ms-marco-docs_3000000_to_3213834\n"
     ]
    }
   ],
   "source": [
    "# load collection\n",
    "collection = sum([ articles for articles in collection_iterator(\"/backup/MS-MARCO/ms-marco-docs.tar.gz\")],[])\n",
    "collection = {x[\"id\"]:x for x in collection}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "999\n"
     ]
    }
   ],
   "source": [
    "# load queries\n",
    "queries = load_TREC_queries(\"/backup/MS-MARCO/msmarco-test2020-queries.tsv\")\n",
    "# load baseline run\n",
    "baseline_run = load_prerank(\"/backup/MS-MARCO/msmarco-doctest2020-top100\", collection, top_k=100)\n",
    "# build ranking set\n",
    "trec_evaluator = TREC_Evaluator(\"\", '/backup/MS-MARCO/trec_eval-9.0.7/trec_eval')\n",
    "baseline_reranking = TestCollectionV2(queries, baseline_run, trec_evaluator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1049519', 'Q0', 'D3466', '1', '-5.57078', 'IndriQueryLikelihood\\n']\n",
      "['1049519', 'Q0', 'D93827', '2', '-5.70761', 'IndriQueryLikelihood\\n']\n",
      "['1049519', 'Q0', 'D3465', '3', '-5.77825', 'IndriQueryLikelihood\\n']\n",
      "['1049519', 'Q0', 'D682582', '4', '-5.84924', 'IndriQueryLikelihood\\n']\n",
      "['1049519', 'Q0', 'D3464', '5', '-5.86327', 'IndriQueryLikelihood\\n']\n",
      "['1049519', 'Q0', 'D3462947', '6', '-5.87248', 'IndriQueryLikelihood\\n']\n",
      "['1049519', 'Q0', 'D682584', '7', '-5.89951', 'IndriQueryLikelihood\\n']\n",
      "['1049519', 'Q0', 'D645071', '8', '-5.91756', 'IndriQueryLikelihood\\n']\n",
      "['1049519', 'Q0', 'D3481480', '9', '-5.92951', 'IndriQueryLikelihood\\n']\n",
      "['1049519', 'Q0', 'D1497292', '10', '-6.00984', 'IndriQueryLikelihood\\n']\n",
      "['1049519', 'Q0', 'D2992778', '11', '-6.02054', 'IndriQueryLikelihood\\n']\n",
      "['1049519', 'Q0', 'D3032335', '12', '-6.06059', 'IndriQueryLikelihood\\n']\n",
      "['1049519', 'Q0', 'D1741758', '13', '-6.06228', 'IndriQueryLikelihood\\n']\n",
      "['1049519', 'Q0', 'D922240', '14', '-6.06627', 'IndriQueryLikelihood\\n']\n",
      "['1049519', 'Q0', 'D2144653', '15', '-6.07913', 'IndriQueryLikelihood\\n']\n",
      "['1049519', 'Q0', 'D2204313', '16', '-6.09977', 'IndriQueryLikelihood\\n']\n",
      "['1049519', 'Q0', 'D834852', '17', '-6.10165', 'IndriQueryLikelihood\\n']\n",
      "['1049519', 'Q0', 'D1808120', '18', '-6.10385', 'IndriQueryLikelihood\\n']\n",
      "['1049519', 'Q0', 'D2195077', '19', '-6.12208', 'IndriQueryLikelihood\\n']\n",
      "['1049519', 'Q0', 'D645069', '20', '-6.12707', 'IndriQueryLikelihood\\n']\n",
      "['1049519', 'Q0', 'D2090866', '21', '-6.12883', 'IndriQueryLikelihood\\n']\n",
      "['1049519', 'Q0', 'D3515018', '22', '-6.13328', 'IndriQueryLikelihood\\n']\n",
      "['1049519', 'Q0', 'D1149309', '23', '-6.13391', 'IndriQueryLikelihood\\n']\n",
      "['1049519', 'Q0', 'D2934979', '24', '-6.13516', 'IndriQueryLikelihood\\n']\n",
      "['1049519', 'Q0', 'D2232921', '25', '-6.14489', 'IndriQueryLikelihood\\n']\n",
      "['1049519', 'Q0', 'D454442', '26', '-6.14535', 'IndriQueryLikelihood\\n']\n",
      "['1049519', 'Q0', 'D2894582', '27', '-6.15426', 'IndriQueryLikelihood\\n']\n",
      "['1049519', 'Q0', 'D1145914', '28', '-6.15619', 'IndriQueryLikelihood\\n']\n",
      "['1049519', 'Q0', 'D3539058', '29', '-6.15673', 'IndriQueryLikelihood\\n']\n",
      "['1049519', 'Q0', 'D940122', '30', '-6.15725', 'IndriQueryLikelihood\\n']\n",
      "['1049519', 'Q0', 'D1641138', '31', '-6.15984', 'IndriQueryLikelihood\\n']\n",
      "['1049519', 'Q0', 'D2646046', '32', '-6.16735', 'IndriQueryLikelihood\\n']\n",
      "['1049519', 'Q0', 'D517396', '33', '-6.17039', 'IndriQueryLikelihood\\n']\n",
      "['1049519', 'Q0', 'D2593649', '34', '-6.17198', 'IndriQueryLikelihood\\n']\n",
      "['1049519', 'Q0', 'D2938596', '35', '-6.17413', 'IndriQueryLikelihood\\n']\n",
      "['1049519', 'Q0', 'D181777', '36', '-6.1747', 'IndriQueryLikelihood\\n']\n",
      "['1049519', 'Q0', 'D1777852', '37', '-6.17976', 'IndriQueryLikelihood\\n']\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-303d91555109>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0melements\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\" \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melements\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0melements\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcollection\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprerank\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0melements\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m<\u001b[0m\u001b[0mtop_k\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m             \u001b[0marticle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcollection\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0melements\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m             prerank[elements[0]].append({\"id\":elements[2], \n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers or slices, not str",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-68e7f0097541>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcollection\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"D2593649\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: list indices must be integers or slices, not str"
     ]
    }
   ],
   "source": [
    "collection[\"D2593649\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
